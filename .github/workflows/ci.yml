name: CI/CD Pipeline
permissions:
  contents: read

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '20'
  DATABASE_URL: postgresql://postgres:postgres_password@localhost:5432/landscape_test?connect_timeout=30&application_name=ci_test
  REDIS_URL: redis://localhost:6379/1
  SECRET_KEY: test-secret-key-not-for-production
  FLASK_ENV: testing
  PYTHONPATH: .

jobs:
  test-backend:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_DB: landscape_test
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres_password
          POSTGRES_INITDB_ARGS: "--auth-host=trust"
        options: >-
          --health-cmd "pg_isready -h localhost -p 5432 -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
          --health-start-period 30s
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 5s
          --health-timeout 3s
          --health-retries 10
          --health-start-period 15s
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install redis-cli
      run: sudo apt-get update && sudo apt-get install -y redis-tools
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: |
          requirements.txt
          requirements-dev.txt
        
    - name: Cache Python dependencies  
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip
          ~/.cache/pip-tools
          ~/.local/share/pip-tools
          ~/.local/lib/python*/site-packages
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt', '**/requirements*.in') }}-v2
        restore-keys: |
          ${{ runner.os }}-pip-
        
    - name: Install dependencies with timeout handling
      run: |
        python -m pip install --upgrade pip
        
        # Install with enhanced timeout and retry logic
        for i in {1..3}; do
          echo "üîÑ Dependency installation attempt $i/3..."
          if timeout 600 pip install -r requirements-dev.txt --timeout=120; then
            echo "‚úÖ Dependencies installed successfully on attempt $i"
            break
          else
            echo "‚ö†Ô∏è Attempt $i failed, retrying in $((i * 15)) seconds..."
            [ $i -lt 3 ] && sleep $((i * 15))
          fi
        done || { echo "‚ùå Failed to install dependencies after 3 attempts"; exit 1; }
        
        # Install additional database driver
        pip install psycopg2-binary --timeout=120
        
    - name: Wait for services with enhanced validation
      run: |
        echo "üîÑ Waiting for database services to be ready..."
        
        # Enhanced PostgreSQL connection testing
        timeout 120 bash -c 'until pg_isready -h localhost -p 5432 -U postgres; do 
          echo "Waiting for PostgreSQL..."; 
          sleep 3; 
        done'
        
        # Enhanced Redis connection testing
        timeout 120 bash -c 'until redis-cli -h localhost -p 6379 ping; do 
          echo "Waiting for Redis..."; 
          sleep 3; 
        done'
        
        # Application-level connection validation
        python -c "
        import psycopg2
        import redis
        import time
        import sys
        
        # Test PostgreSQL with retry logic
        for attempt in range(15):
            try:
                conn = psycopg2.connect(
                    'postgresql://postgres:postgres_password@localhost:5432/landscape_test'
                )
                conn.close()
                print('‚úÖ PostgreSQL connection successful')
                break
            except Exception as e:
                print(f'PostgreSQL attempt {attempt+1}/15: {e}')
                if attempt < 14:
                    time.sleep(5)
                else:
                    print('‚ùå PostgreSQL connection failed after 15 attempts')
                    sys.exit(1)
        
        # Test Redis with retry logic  
        for attempt in range(15):
            try:
                r = redis.from_url('redis://localhost:6379/1')
                r.ping()
                print('‚úÖ Redis connection successful')
                break
            except Exception as e:
                print(f'Redis attempt {attempt+1}/15: {e}')
                if attempt < 14:
                    time.sleep(5)
                else:
                    print('‚ùå Redis connection failed after 15 attempts')
                    sys.exit(1)
        
        print('üéâ All database services are ready and validated')
        "
        
    - name: Validate environment configuration
      run: |
        echo "üîç Validating environment variable configuration..."
        
        # Check required variables are set
        python -c "
        import os
        import sys
        
        required_vars = [
            'DATABASE_URL',
            'REDIS_URL', 
            'SECRET_KEY',
            'FLASK_ENV',
            'PYTHONPATH'
        ]
        
        missing_vars = []
        for var in required_vars:
            if not os.getenv(var):
                missing_vars.append(var)
        
        if missing_vars:
            print(f'‚ùå Missing required environment variables: {missing_vars}')
            sys.exit(1)
        else:
            print('‚úÖ All required environment variables are set')
            
        # Validate database URL format
        db_url = os.getenv('DATABASE_URL')
        if 'connect_timeout' not in db_url:
            print('‚ö†Ô∏è DATABASE_URL missing connect_timeout parameter')
        if 'application_name' not in db_url:
            print('‚ö†Ô∏è DATABASE_URL missing application_name parameter')
            
        print('‚úÖ Environment variable validation complete')
        "
        
    - name: Run database migrations
      env:
        DATABASE_URL: postgresql://postgres:postgres_password@localhost:5432/landscape_test?connect_timeout=30&application_name=ci_test
        PYTHONPATH: .
      run: |
        flask --app src.main db upgrade
        
    - name: Test backend with SQLite
      env:
        PYTHONPATH: .
        FLASK_ENV: testing
      run: |
        python -m pytest tests/ -v --tb=short --maxfail=5
        
    - name: Test backend with PostgreSQL
      env:
        DATABASE_URL: postgresql://postgres:postgres_password@localhost:5432/landscape_test?connect_timeout=30&application_name=ci_test
        REDIS_URL: redis://localhost:6379/1
        SECRET_KEY: test-secret-key-not-for-production
        PYTHONPATH: .
        FLASK_ENV: testing
      run: |
        python -m pytest tests/ -v --tb=short --maxfail=5

  test-frontend:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
        
    - name: Install dependencies
      run: |
        cd frontend
        npm ci --legacy-peer-deps
        
    - name: Run security audit
      run: |
        cd frontend
        npm audit --audit-level=moderate || echo "Security vulnerabilities found"
        
    - name: Lint frontend
      run: |
        cd frontend
        npm run lint
        
    - name: Build frontend
      run: |
        cd frontend
        npm run build
        
    - name: Test frontend with coverage
      run: |
        cd frontend
        npm run test:coverage -- --watchAll=false --ci --coverage --passWithNoTests
        
    - name: Check coverage thresholds
      run: |
        cd frontend
        if [ -f coverage/coverage-summary.json ]; then
          echo "Coverage summary found, checking thresholds..."
          
          # Install jq if not available
          which jq || (echo "Installing jq..." && sudo apt-get update && sudo apt-get install -y jq)
          
          # Extract coverage percentages (use default values if not found)
          LINES=$(cat coverage/coverage-summary.json | jq -r '.total.lines.pct // 0')
          FUNCTIONS=$(cat coverage/coverage-summary.json | jq -r '.total.functions.pct // 0')
          BRANCHES=$(cat coverage/coverage-summary.json | jq -r '.total.branches.pct // 0')
          STATEMENTS=$(cat coverage/coverage-summary.json | jq -r '.total.statements.pct // 0')
          
          echo "Coverage Report:"
          echo "Lines: ${LINES}%"
          echo "Functions: ${FUNCTIONS}%"
          echo "Branches: ${BRANCHES}%"
          echo "Statements: ${STATEMENTS}%"
          
          # For now, just report coverage without failing CI
          # TODO: Implement proper thresholds once tests are comprehensive
          echo "‚úÖ Coverage reported successfully"
        else
          echo "No coverage report found, skipping threshold check"
        fi
        
    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      if: success()
      with:
        name: frontend-build
        path: frontend/dist/
        retention-days: 7
        
    - name: Upload coverage artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: frontend-coverage
        path: |
          frontend/coverage/
          !frontend/coverage/tmp/
        retention-days: 30

  code-quality:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Install redis-cli
      run: sudo apt-get update && sudo apt-get install -y redis-tools
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: |
          requirements.txt
          requirements-dev.txt
        
    - name: Cache linting tools
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-linting-tools-v1
        restore-keys: |
          ${{ runner.os }}-linting-tools-
          
    - name: Cache pre-commit environments
      uses: actions/cache@v4
      with:
        path: ~/.cache/pre-commit
        key: ${{ runner.os }}-pre-commit-${{ hashFiles('**/.pre-commit-config.yaml') }}
        restore-keys: |
          ${{ runner.os }}-pre-commit-
        
    - name: Install linting tools with timeout handling
      run: |
        python -m pip install --upgrade pip
        
        # Install linting tools with up to 3 attempts, each with a 300-second timeout
        for i in {1..3}; do
          echo "üîÑ Linting tools installation attempt $i/3..."
          if timeout 300 pip install --timeout=120 black==24.3.0 flake8==7.0.0 isort==5.13.2 bandit==1.7.5 safety==3.0.1; then
            echo "‚úÖ Linting tools installed successfully on attempt $i"
            break
          else
            echo "‚ö†Ô∏è Attempt $i failed, retrying in $((i * 10)) seconds..."
            [ $i -lt 3 ] && sleep $((i * 10))
          fi
        done || { echo "‚ùå Failed to install linting tools after 3 attempts"; exit 1; }

    - name: Check Python formatting
      run: |
        black --check --line-length 88 src/ tests/ --diff
        
    - name: Check import sorting  
      run: |
        isort --check-only --profile black src/ tests/ --diff

    - name: Run Python linting
      run: |
        flake8 src/ tests/ --max-line-length=88 --extend-ignore=E203,W503,F401,F403,E402,C901,W291 --max-complexity=25
        
    - name: Run security linting
      run: |
        bandit -r src/ -f json -o bandit-report.json || true
        
    - name: Check Python security
      run: |
        safety check --json || echo "Security vulnerabilities found"
        
    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports
        path: bandit-report.json
        retention-days: 30

  integration-tests:
    needs: [test-backend, test-frontend]
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_DB: landscape_integration_test
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres_password
          POSTGRES_INITDB_ARGS: "--auth-host=trust"
        options: >-
          --health-cmd "pg_isready -h localhost -p 5432 -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
          --health-start-period 30s
        ports:
          - 5432:5432
          
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 5s
          --health-timeout 3s
          --health-retries 10
          --health-start-period 15s
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install redis-cli
      run: sudo apt-get update && sudo apt-get install -y redis-tools
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: |
          requirements.txt
          requirements-dev.txt
        
    - name: Install dependencies with timeout handling
      run: |
        python -m pip install --upgrade pip
        
        # Install with enhanced timeout and retry logic
        for i in {1..3}; do
          echo "üîÑ Dependencies installation attempt $i/3..."
          if timeout 600 pip install -r requirements-dev.txt --timeout=120; then
            echo "‚úÖ Dependencies installed successfully on attempt $i"
            break
          else
            echo "‚ö†Ô∏è Attempt $i failed, retrying in $((i * 15)) seconds..."
            [ $i -lt 3 ] && sleep $((i * 15))
          fi
        done || { echo "‚ùå Failed to install dependencies after 3 attempts"; exit 1; }
        
        # Install additional packages with timeout
        pip install psycopg2-binary requests --timeout=120
        
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
        
    - name: Install frontend dependencies
      run: |
        cd frontend
        npm ci --legacy-peer-deps
        
    - name: Build frontend
      run: |
        cd frontend
        npm run build
        
    - name: Run integration tests
      env:
        DATABASE_URL: postgresql://postgres:postgres_password@localhost:5432/landscape_integration_test?connect_timeout=30&application_name=integration_test
        REDIS_URL: redis://localhost:6379/1
        SECRET_KEY: integration-test-secret-key
        FLASK_ENV: testing
        PYTHONPATH: .
      run: |
        # Enhanced service health validation
        echo "Validating service connections..."
        python -c "
        import psycopg2
        import redis
        import time
        
        # Test PostgreSQL connection
        for i in range(10):
            try:
                conn = psycopg2.connect('$DATABASE_URL')
                conn.close()
                print('‚úÖ PostgreSQL connection successful')
                break
            except Exception as e:
                print(f'PostgreSQL attempt {i+1}: {e}')
                time.sleep(2)
        else:
            raise Exception('Failed to connect to PostgreSQL')
            
        # Test Redis connection  
        for i in range(10):
            try:
                r = redis.from_url('$REDIS_URL')
                r.ping()
                print('‚úÖ Redis connection successful')
                break
            except Exception as e:
                print(f'Redis attempt {i+1}: {e}')
                time.sleep(2)
        else:
            raise Exception('Failed to connect to Redis')
        "
        
        # Setup database
        flask --app src.main db upgrade
        
        # Start backend in background with logging
        echo "Starting backend application..."
        python src/main.py > backend.log 2>&1 &
        BACKEND_PID=$!
        
        # Wait for backend to start with enhanced timeout
        echo "Waiting for backend to be ready..."
        timeout 60 bash -c 'until curl -f http://localhost:5000/health; do sleep 2; done'
        
        echo "Backend is ready, running integration tests..."
        
        # Test API endpoints with error handling
        set -e  # Exit on any error
        
        echo "Testing health endpoint..."
        curl -f http://localhost:5000/health
        
        echo "Testing dashboard stats..."
        curl -f http://localhost:5000/api/dashboard/stats
        
        echo "Testing CRUD operations..."
        
        # Test supplier creation
        SUPPLIER_RESPONSE=$(curl -X POST http://localhost:5000/api/suppliers \
          -H "Content-Type: application/json" \
          -d '{"name":"Test Supplier","contact_person":"John Doe","email":"john@test.com","phone":"123-456-7890","address":"123 Test St"}')
        
        echo "Supplier creation response: $SUPPLIER_RESPONSE"
        
        # Test supplier listing
        echo "Testing supplier listing..."
        curl -f http://localhost:5000/api/suppliers
        
        echo "‚úÖ All integration tests passed"
        
        # Collect application metrics
        echo "Backend application metrics:" >> integration-artifacts/summary.txt
        curl -s http://localhost:5000/health >> integration-artifacts/summary.txt 2>&1 || echo "Health endpoint not available" >> integration-artifacts/summary.txt
        
        # Cleanup
        echo "Stopping backend application..."
        kill $BACKEND_PID || true
        wait $BACKEND_PID 2>/dev/null || true
        
    - name: Collect integration test artifacts
      if: always()
      run: |
        # Collect logs and any artifacts
        mkdir -p integration-artifacts
        
        # Copy backend log if it exists
        if [ -f backend.log ]; then
          cp backend.log integration-artifacts/
          echo "Backend log file size: $(wc -l < backend.log) lines" >> integration-artifacts/summary.txt
        else
          echo "No backend log file found" >> integration-artifacts/summary.txt
        fi
        
        # Create comprehensive test summary
        echo "Integration Test Summary" > integration-artifacts/summary.txt
        echo "Timestamp: $(date)" >> integration-artifacts/summary.txt
        echo "Workflow: ${{ github.workflow }}" >> integration-artifacts/summary.txt
        echo "Run ID: ${{ github.run_id }}" >> integration-artifacts/summary.txt
        echo "Backend PID: ${BACKEND_PID:-unknown}" >> integration-artifacts/summary.txt
        echo "Exit Status: ${?}" >> integration-artifacts/summary.txt
        
        # Collect system information
        echo "" >> integration-artifacts/summary.txt
        echo "System Information:" >> integration-artifacts/summary.txt
        echo "Disk usage:" >> integration-artifacts/summary.txt
        df -h >> integration-artifacts/summary.txt 2>&1 || true
        echo "Memory usage:" >> integration-artifacts/summary.txt
        free -h >> integration-artifacts/summary.txt 2>&1 || true
        echo "Process count:" >> integration-artifacts/summary.txt
        ps aux | wc -l >> integration-artifacts/summary.txt 2>&1 || true
        
    - name: Upload integration test artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: integration-test-artifacts
        path: integration-artifacts/
        retention-days: 7

  docker-build:
    needs: [test-backend, test-frontend, code-quality]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Build Docker images
      run: |
        # Validate docker compose configuration
        docker compose config > /dev/null
        
        # Build images
        docker compose build
        
        # Test with health checks
        docker compose up -d --wait --wait-timeout 300
        
        # Test backend endpoint (backend runs on port 5000)
        timeout 30 bash -c 'until curl -f http://localhost:5000/health; do sleep 2; done'
        
        # Test frontend and API through nginx (port 80)
        timeout 30 bash -c 'until curl -f http://localhost:80; do sleep 2; done'
        timeout 30 bash -c 'until curl -f http://localhost:80/health; do sleep 2; done'
        
        # Cleanup
        docker compose down -v

  deploy:
    needs: [integration-tests, docker-build, deepsource, monitoring]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - name: Deploy notification
      run: |
        echo "üöÄ All checks passed! Ready for deployment"
        echo "üìä Test Results: All tests passed"
        echo "üîß Next steps: Manual review and approval for production deployment"

  deepsource:
    runs-on: ubuntu-latest
    needs: [test-backend, test-frontend]
    if: always() && (needs.test-backend.result == 'success' || needs.test-frontend.result == 'success')
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
        
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: |
          requirements.txt
          requirements-dev.txt
        
    - name: Install Python dependencies with timeout handling
      run: |
        python -m pip install --upgrade pip
        
        # Install with enhanced timeout and retry logic
        for i in {1..3}; do
          echo "üîÑ Dependencies installation attempt $i/3..."
          if timeout 600 pip install -r requirements-dev.txt --timeout=120; then
            echo "‚úÖ Dependencies installed successfully on attempt $i"
            break
          else
            echo "‚ö†Ô∏è Attempt $i failed, retrying in $((i * 15)) seconds..."
            [ $i -lt 3 ] && sleep $((i * 15))
          fi
        done || { echo "‚ùå Failed to install dependencies after 3 attempts"; exit 1; }
        
    - name: Generate test coverage
      env:
        PYTHONPATH: .
        FLASK_ENV: testing
      run: |
        python -m pytest tests/ --cov=src --cov-report=xml --cov-report=html --tb=short
        
    - name: Upload coverage to DeepSource
      env:
        DEEPSOURCE_DSN: ${{ secrets.DEEPSOURCE_DSN }}
      run: |
        if [ ! -z "$DEEPSOURCE_DSN" ]; then
          echo "üìä Uploading coverage to DeepSource..."
          
          # Install DeepSource CLI with error handling
          if curl -fsSL https://deepsource.io/cli | sh; then
            echo "‚úÖ DeepSource CLI installed successfully"
            
            # Report coverage with error handling
            if ./bin/deepsource report --analyzer test-coverage --key python --value-file ./coverage.xml; then
              echo "‚úÖ Coverage reported to DeepSource successfully"
            else
              echo "‚ö†Ô∏è Failed to upload coverage to DeepSource"
              echo "This may be due to network issues or invalid DSN"
              exit 0  # Don't fail the entire pipeline for DeepSource issues
            fi
          else
            echo "‚ö†Ô∏è Failed to install DeepSource CLI"
            echo "This may be due to network connectivity issues"
            echo "Coverage data is still available as artifacts"
            exit 0  # Don't fail the entire pipeline for installation issues
          fi
        else
          echo "‚ö†Ô∏è DeepSource DSN not configured, skipping coverage upload"
          echo "‚ÑπÔ∏è To enable DeepSource integration, add DEEPSOURCE_DSN to repository secrets"
          echo "‚ÑπÔ∏è Visit https://deepsource.io to get your DSN token"
        fi
        
    - name: Upload coverage artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: coverage-reports
        path: |
          coverage.xml
          htmlcov/
        retention-days: 30

  monitoring:
    runs-on: ubuntu-latest
    needs: [test-backend, test-frontend, code-quality, integration-tests, deepsource]
    if: always()
    steps:
    - name: Pipeline status monitoring and analytics
      run: |
        echo "=== CI/CD Pipeline Status Summary ==="
        echo "Timestamp: $(date)"
        echo "Repository: ${{ github.repository }}"
        echo "Branch: ${{ github.ref_name }}"
        echo "Commit: ${{ github.sha }}"
        echo "Workflow: ${{ github.workflow }}"
        echo "Run ID: ${{ github.run_id }}"
        echo "Run Number: ${{ github.run_number }}"
        echo ""
        
        echo "Job Results:"
        echo "  test-backend: ${{ needs.test-backend.result }}"
        echo "  test-frontend: ${{ needs.test-frontend.result }}"
        echo "  code-quality: ${{ needs.code-quality.result }}"
        echo "  integration-tests: ${{ needs.integration-tests.result }}"
        echo "  deepsource: ${{ needs.deepsource.result }}"
        echo ""
        
        # Calculate success rate with safer arithmetic
        TOTAL=5
        SUCCESS=0
        FAILED_JOBS=""
        
        if [[ "${{ needs.test-backend.result }}" == "success" ]]; then
          SUCCESS=$((SUCCESS + 1))
        else
          FAILED_JOBS="${FAILED_JOBS} test-backend(${{ needs.test-backend.result }})"
        fi
        
        if [[ "${{ needs.test-frontend.result }}" == "success" ]]; then
          SUCCESS=$((SUCCESS + 1))
        else
          FAILED_JOBS="${FAILED_JOBS} test-frontend(${{ needs.test-frontend.result }})"
        fi
        
        if [[ "${{ needs.code-quality.result }}" == "success" ]]; then
          SUCCESS=$((SUCCESS + 1))
        else
          FAILED_JOBS="${FAILED_JOBS} code-quality(${{ needs.code-quality.result }})"
        fi
        
        if [[ "${{ needs.integration-tests.result }}" == "success" ]]; then
          SUCCESS=$((SUCCESS + 1))
        else
          FAILED_JOBS="${FAILED_JOBS} integration-tests(${{ needs.integration-tests.result }})"
        fi
        
        if [[ "${{ needs.deepsource.result }}" == "success" ]]; then
          SUCCESS=$((SUCCESS + 1))
        else
          FAILED_JOBS="${FAILED_JOBS} deepsource(${{ needs.deepsource.result }})"
        fi
        
        SUCCESS_RATE=$((SUCCESS * 100 / TOTAL))
        echo "Success Rate: ${SUCCESS_RATE}% (${SUCCESS}/${TOTAL})"
        
        if [[ -n "$FAILED_JOBS" ]]; then
          echo "Failed Jobs:${FAILED_JOBS}"
        fi
        echo ""
        
        # Pipeline health assessment
        if [ $SUCCESS_RATE -eq 100 ]; then
          echo "‚úÖ All core pipeline jobs completed successfully!"
          echo "üöÄ Pipeline ready for deployment"
          echo "üìà Pipeline Health: EXCELLENT"
        elif [ $SUCCESS_RATE -ge 80 ]; then
          echo "‚ö†Ô∏è Pipeline mostly successful with some issues"
          echo "üìä Review failed jobs before deployment"
          echo "üìà Pipeline Health: GOOD"
        elif [ $SUCCESS_RATE -ge 60 ]; then
          echo "‚ö†Ô∏è Pipeline has moderate failures"
          echo "üîç Investigation required for failed jobs"
          echo "üìà Pipeline Health: FAIR"
        else
          echo "‚ùå Pipeline has significant failures"
          echo "üîß Address failing jobs before proceeding"
          echo "üìà Pipeline Health: POOR"
        fi
        
        # Performance and reliability metrics
        echo ""
        echo "=== Pipeline Metrics ==="
        echo "Trigger: ${{ github.event_name }}"
        echo "Actor: ${{ github.actor }}"
        echo "Ref Type: ${{ github.ref_type }}"
        
        # Export comprehensive metrics for potential future use
        echo "success_rate=${SUCCESS_RATE}" >> $GITHUB_OUTPUT
        echo "total_jobs=${TOTAL}" >> $GITHUB_OUTPUT
        echo "successful_jobs=${SUCCESS}" >> $GITHUB_OUTPUT
        echo "failed_jobs=$((TOTAL - SUCCESS))" >> $GITHUB_OUTPUT
        echo "pipeline_health=$(if [ $SUCCESS_RATE -eq 100 ]; then echo 'EXCELLENT'; elif [ $SUCCESS_RATE -ge 80 ]; then echo 'GOOD'; elif [ $SUCCESS_RATE -ge 60 ]; then echo 'FAIR'; else echo 'POOR'; fi)" >> $GITHUB_OUTPUT
        
        # Always exit successfully - monitoring is informational only
        echo ""
        echo "üìà Monitoring completed successfully"
        echo "üìä Metrics exported to GitHub outputs for trend analysis"
        exit 0